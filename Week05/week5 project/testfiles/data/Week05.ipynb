{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8615ecf-ac4f-4833-861f-c52d0e65da90",
   "metadata": {},
   "source": [
    "## FinTech 545 - Project Week 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e383536-68d7-42a1-81f6-3becb4c0286e",
   "metadata": {},
   "source": [
    "## Xuanang Zhu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fb764-d5f9-4f48-aef2-67cf3aea8d0f",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcae3a-d750-42d0-847b-3cb3479fa5f7",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616946b-01a6-4096-a983-2b3c503502c0",
   "metadata": {},
   "source": [
    "Covariance estimation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0607d26c-1b32-4738-bc4c-03e7a7fa6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Function to calculate the covariance matrix for the dataframe that does not have the entire data.\n",
    "When skipRow is true, use all the rows which have values. When it's false, use pairwise.\n",
    "func can be np.cov (covariance) and np.corrcoef (correlation)\n",
    "'''\n",
    "def missing_cov_corr(df, skipRow=True, func=np.cov):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    m, n = df.shape\n",
    "    missing_rows = df.isnull().any(axis=1).sum()\n",
    "\n",
    "    # If there is no missing rows, simply calculate the covariance matrix.\n",
    "    if not missing_rows:\n",
    "        cov = func(df.T)\n",
    "        print(cov)\n",
    "    else:\n",
    "        # skipRow is True, apply the method on the rows that have all the data.\n",
    "        if skipRow:\n",
    "            # Drop the rows that is not of whole data\n",
    "            df = df.dropna(axis=0, how='any')\n",
    "            cov = func(df.T)\n",
    "        # skipRow is True, apply the pairwise method.\n",
    "        else:\n",
    "            out = np.empty((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1):\n",
    "                    # Select only rows without missing values in either column i or j\n",
    "                    valid_rows = df.iloc[:, [i, j]].dropna().index\n",
    "\n",
    "                    if not valid_rows.empty:\n",
    "                        cov_ij = func(df.iloc[valid_rows, [i, j]], rowvar=False)[0, 1]\n",
    "                        out[i, j] = cov_ij\n",
    "                        out[j, i] = cov_ij\n",
    "                        cov = out\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaa8ae-d09d-46c0-b2c7-b0f4f823186d",
   "metadata": {},
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04d6166b-ef78-432d-aa1f-5209401b2396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_1.1\n",
    "\n",
    "df = pd.read_csv('test1.csv')\n",
    "out1 = pd.read_csv('testout_1.1.csv')\n",
    "\n",
    "# Calculate covariance matrix skipping rows\n",
    "res1 = missing_cov_corr(df, skipRow=True, func=np.cov)\n",
    "close_elements = np.isclose(out1, res1, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 1.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "653584ea-c6f4-4d56-b9f6-57ad3cde7792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_1.2\n",
    "\n",
    "out2 = pd.read_csv('testout_1.2.csv')\n",
    "\n",
    "# Calculate correlation matrix skipping rows\n",
    "res2 = missing_cov_corr(df, skipRow=True, func=np.corrcoef)\n",
    "close_elements = np.isclose(out2, res2, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 1.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aa856d29-c59b-4b5b-8842-9eef882e9319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_1.3\n",
    "\n",
    "out3 = pd.read_csv('testout_1.3.csv')\n",
    "\n",
    "# Calculate covariance matrix pairwise\n",
    "res3 = missing_cov_corr(df, skipRow=False, func=np.cov)\n",
    "close_elements = np.isclose(out3, res3, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 1.3 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "02d47751-a9fe-4a74-9eec-d91db02c8348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_1.4\n",
    "\n",
    "out4 = pd.read_csv('testout_1.4.csv')\n",
    "\n",
    "# Calculate covariance matrix pairwise\n",
    "res4 = missing_cov_corr(df, skipRow=False, func=np.corrcoef)\n",
    "close_elements = np.isclose(out4, res4, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 1.4 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7eb0acc5-7455-4ac6-a921-da4223db6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that calculates the EW covariance and correlation.\n",
    "Func take the parameter of 'cov' and 'corr'\n",
    "'''\n",
    "def ew_cov_corr(df, lmbd, func='cov'):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "        \n",
    "    if func not in ['cov', 'corr']:\n",
    "        raise ValueError(f'The func parameter must be \"cov\" or \"corr\", got {func} instead.')\n",
    "    \n",
    "    # Center the data - to calculate the covariance matrix.\n",
    "    df -= df.mean(axis=0)\n",
    "        \n",
    "    m, n = df.shape\n",
    "    wts = np.empty(m)\n",
    "    \n",
    "    # Setting weights for prior observation\n",
    "    for i in range(m):\n",
    "        wts[i] = (1 - lmbd) * lmbd ** (m - i - 1)\n",
    "        \n",
    "    # Normalizing the weights\n",
    "    wts /= np.sum(wts)\n",
    "    wts = wts.reshape(-1, 1)\n",
    "    if func == 'cov':   \n",
    "        res = (wts * df).T @ df\n",
    "        \n",
    "    elif func == 'corr':\n",
    "        res = (wts * df).T @ df\n",
    "        # Calculate the standard deviations (square root of variances along the diagonal)\n",
    "        std_devs = np.sqrt(np.diag(res))\n",
    "\n",
    "        # Convert the covariance matrix to a correlation matrix\n",
    "        res /= np.outer(std_devs, std_devs)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b2729a0d-949a-4a25-8dd6-90a259d7d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_2.1\n",
    "df = pd.read_csv('test2.csv')\n",
    "\n",
    "out1 = pd.read_csv('testout_2.1.csv')\n",
    "res1 = ew_cov_corr(df, 0.97, func='cov')\n",
    "close_elements = np.isclose(out1, res1, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 2.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b25e0804-40c1-4581-a756-1746cc6d497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_2.2\n",
    "out2 = pd.read_csv('testout_2.2.csv')\n",
    "res2 = ew_cov_corr(df, 0.94, func='corr')\n",
    "close_elements = np.isclose(out2, res2, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 2.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd663e61-c56e-4cd7-918f-7e8d607182b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_2.3\n",
    "out3 = pd.read_csv('testout_2.3.csv')\n",
    "\n",
    "cov = ew_cov_corr(df, 0.97, func='cov')\n",
    "sd1 = np.sqrt(np.diag(cov))\n",
    "cov  = ew_cov_corr(df, 0.94, func='cov')\n",
    "sd = 1 / np.sqrt(np.diag(cov))\n",
    "res3 = np.diag(sd1) @ np.diag(sd) @ cov @ np.diag(sd) @ np.diag(sd1)\n",
    "\n",
    "close_elements = np.isclose(out3, res3, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 2.3 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ecaf3-8645-4ce1-a469-60bae157a026",
   "metadata": {},
   "source": [
    "Non-PSD fixes for correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23a259a3-d523-42ae-a30e-daac4e48ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Near-PSD covariance and correlation.\n",
    "'''\n",
    "def near_psd(df, epsilon=0.0):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    invSD = None\n",
    "\n",
    "    # If the matrix is the covariance matrix, convert it to correlation matrix first.\n",
    "    if not np.allclose(np.diag(df), 1.0, rtol=1e-03):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(df)))\n",
    "        df = invSD @ df @ invSD\n",
    "        # # Calculate the standard deviations (square root of variances along the diagonal)\n",
    "        # std_devs = np.sqrt(np.diag(df))\n",
    "        # # Convert the covariance matrix to a correlation matrix\n",
    "        # df /= np.outer(std_devs, std_devs)\n",
    "    \n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(df)\n",
    "    \n",
    "    # Update the negative eigenvalues to 0.\n",
    "    eigenvalues = np.maximum(eigenvalues, epsilon)\n",
    "    \n",
    "    # Construct the diagonal scaling matrix\n",
    "    S = 1 / (eigenvectors * eigenvectors @ eigenvalues)\n",
    "    S = np.diag(np.sqrt(S))\n",
    "    #T = np.diag(1.0 / np.sqrt(np.sum(eigenvectors**2 * eigenvalues, axis=0)))\n",
    "    l = np.diag(np.sqrt(eigenvalues))\n",
    "    B = S @ eigenvectors @ l\n",
    "    df = B @ B.T\n",
    "    \n",
    "    # Add back the variance\n",
    "    if invSD is not None:\n",
    "        invSD = np.diag(1 / np.diag(invSD))\n",
    "        df = invSD @ df @ invSD\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a10e950-f592-4693-93f8-4fdd3c759d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_3.1\n",
    "df = pd.read_csv('testout_1.3.csv')\n",
    "\n",
    "out1 = pd.read_csv('testout_3.1.csv')\n",
    "res1 = near_psd(df)\n",
    "close_elements = np.isclose(out1, res1, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 3.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "492c67a2-3e51-4090-b805-dc6f11907fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_3.2\n",
    "df = pd.read_csv('testout_1.4.csv')\n",
    "\n",
    "out2 = pd.read_csv('testout_3.2.csv')\n",
    "res2 = near_psd(df)\n",
    "close_elements = np.isclose(out2, res2, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 3.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b6221cd5-4786-4a3a-9954-6b5b5dc13fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Higham-PSD covariance and correlation.\n",
    "'''\n",
    "def higham_psd(df, W=None, epsilon=1e-9, maxIter=100, tol=1e-9):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "    \n",
    "    m, n = df.shape\n",
    "    \n",
    "    # Generate the identity matrix.\n",
    "    if W is None:\n",
    "        W = np.eye(m)\n",
    "        \n",
    "    deltaS = 0    \n",
    "    invSD = None\n",
    "    \n",
    "    Yk = np.copy(df)\n",
    "\n",
    "    # If the matrix is the covariance matrix, convert it to correlation matrix first.\n",
    "    if not np.allclose(np.diag(Yk), 1.0, rtol=1e-03):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(Yk)))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "        \n",
    "    Yo = np.copy(Yk)\n",
    "    norml = np.finfo(np.float64).max\n",
    "    i = 1\n",
    "    \n",
    "    while i <= maxIter:\n",
    "        Rk = Yk - deltaS\n",
    "        \n",
    "        # Ps update\n",
    "        Xk = getPS(Rk, W)\n",
    "        deltaS = Xk - Rk\n",
    "        # Pu update\n",
    "        Yk = getPu(Xk)\n",
    "        # Get Norm\n",
    "        norm = wgtNorm(Yk-Yo, W)\n",
    "        #Smallest Eigenvalue\n",
    "        minEigVal = np.min(np.real(np.linalg.eigvals(Yk)))\n",
    "        \n",
    "        if norm - norml < tol and minEigVal > -epsilon:\n",
    "            break\n",
    "        \n",
    "        norml = norm\n",
    "        i += 1\n",
    "\n",
    "    if i < maxIter:\n",
    "        print(\"Converged in {} iterations.\".format(i))\n",
    "    else:\n",
    "        print(\"Convergence failed after {} iterations\".format(i-1))\n",
    "    \n",
    "    # Add back the variance\n",
    "    if invSD is not None:\n",
    "        invSD = np.diag(1 / np.diag(invSD))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "        \n",
    "    return Yk\n",
    "\n",
    "'''\n",
    "Helper functions\n",
    "'''\n",
    "def getAplus(A):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    eigenvalues = np.diag(np.maximum(eigenvalues, 0))\n",
    "    return eigenvectors @ eigenvalues @ eigenvectors.T\n",
    "\n",
    "def getPS(A, W):\n",
    "    W05 = np.sqrt(W)\n",
    "    iW = np.linalg.inv(W05)\n",
    "    return (iW @ getAplus(W05 @ A @ W05) @ iW)\n",
    "\n",
    "def getPu(A):\n",
    "    A = np.copy(A)  # Work on a copy to avoid modifying the original matrix\n",
    "    np.fill_diagonal(A, 1)\n",
    "    return A\n",
    "\n",
    "def wgtNorm(A, W):\n",
    "    W05 = np.sqrt(W)\n",
    "    W05 = W05 @ A @ W05\n",
    "    return np.sum(W05 * W05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b34c0540-b136-45ea-a9d2-f95c37be4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 20 iterations.\n",
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_3.3\n",
    "df = pd.read_csv('testout_1.3.csv')\n",
    "\n",
    "out3 = pd.read_csv('testout_3.3.csv')\n",
    "res3 = higham_psd(df)\n",
    "close_elements = np.isclose(out3, res3, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 3.3 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83f0b821-e2f1-4c6d-a663-b1131513f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 1 iterations.\n",
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_3.4\n",
    "df = pd.read_csv('testout_1.4.csv')\n",
    "\n",
    "out4 = pd.read_csv('testout_3.4.csv')\n",
    "res4 = higham_psd(df)\n",
    "close_elements = np.isclose(out4, res4, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 3.4 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77cf6fbd-557d-4896-b94f-0d8347e02381",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cholesky Factorization\n",
    "'''\n",
    "def chol_psd(df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "    \n",
    "    m, n = df.shape\n",
    "    root = np.zeros((m, n))  # Initialize the root matrix within the function\n",
    "    \n",
    "    for j in range(m):\n",
    "        s = 0.0\n",
    "        # If we are not on the first column, calculate the dot product of the preceeding row values.\n",
    "        if j >= 0:\n",
    "            s =  np.dot(root[j, :j], root[j, :j])\n",
    "            \n",
    "            # Diagonal element\n",
    "            temp = df.iloc[j, j] - s\n",
    "            if -1e-8 <= temp <= 0:\n",
    "                temp = 0.0\n",
    "            root[j, j] = np.sqrt(max(temp, 0))\n",
    "            \n",
    "            if root[j, j] == 0.0:\n",
    "                root[j, (j+1):n] = 0.0\n",
    "            else:\n",
    "                ir = 1.0 / root[j, j]\n",
    "                for i in range(j+1, m):\n",
    "                    s = np.dot(root[i, :j], root[j, :j])\n",
    "                    root[i, j] = (df.iloc[i, j] - s) * ir\n",
    "                    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f442d83-51f4-4bf8-bfd6-d32b76ba9123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_4.1\n",
    "df = pd.read_csv('testout_3.1.csv')\n",
    "\n",
    "out = pd.read_csv('testout_4.1.csv')\n",
    "res = chol_psd(df)\n",
    "close_elements = np.isclose(out, res, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 4.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c527dc-6621-429a-b4ee-1bf6427f4534",
   "metadata": {},
   "source": [
    "Simulation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bd4b3f1b-080f-46ac-826b-f49ee15824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multivariate Normal Simulation\n",
    "'''\n",
    "def simulateNormal(N, df, mean=None, seed=1234, fixMethod=near_psd):\n",
    "    # Error Checking\n",
    "    m, n = df.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    \n",
    "    # Initialize the output\n",
    "    out = np.zeros((N, n))\n",
    "    \n",
    "    # Set mean\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "    \n",
    "    # Set the seed to make sure the value is the same each time.\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(df)\n",
    "    # If the covariance is not PSD, try to fix it\n",
    "    if min(eigenvalues) < 0:\n",
    "        df = fixMethod(df)\n",
    "        \n",
    "    # Take the root (cholesky factorization)\n",
    "    l = chol_psd(df)\n",
    "    \n",
    "    # Generate random standard normals\n",
    "    rand_normals = np.random.normal(0.0, 1.0, size=(N, n))\n",
    "    \n",
    "    # Apply the Cholesky root and plus the mean to the random normals\n",
    "    out = np.dot(rand_normals, l.T) + mean\n",
    "    \n",
    "    return out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c6118293-8371-4485-978e-9cb226753589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.1\n",
    "df = pd.read_csv('test5_1.csv')\n",
    "\n",
    "out = pd.read_csv('testout_5.1.csv')\n",
    "sim = simulateNormal(100000, df)\n",
    "res = np.cov(sim)\n",
    "\n",
    "# atol is the absolute tolerance parameter - set to a lower tolerance since the simulation is random.\n",
    "close_elements = np.isclose(out, res, atol=1e-3)\n",
    "\n",
    "\n",
    "# Test 5.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3afeaec0-9c3d-41db-9008-364a10c52bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.2\n",
    "df = pd.read_csv('test5_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout_5.2.csv')\n",
    "sim = simulateNormal(100000, df)\n",
    "res = np.cov(sim)\n",
    "\n",
    "# atol is the absolute tolerance parameter - set to a lower tolerance since the simulation is random.\n",
    "close_elements = np.isclose(out, res, atol=1e-3)\n",
    "\n",
    "\n",
    "# Test 5.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "630cacf8-c9c7-4fa7-9122-520fd48bbe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.3\n",
    "df = pd.read_csv('test5_3.csv')\n",
    "\n",
    "out = pd.read_csv('testout_5.3.csv')\n",
    "sim = simulateNormal(100000, df, fixMethod=near_psd)\n",
    "res = np.cov(sim)\n",
    "\n",
    "# atol is the absolute tolerance parameter - set to a lower tolerance since the simulation is random.\n",
    "close_elements = np.isclose(out, res, atol=1e-3)\n",
    "\n",
    "\n",
    "# Test 5.3 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "87277133-af03-4009-9b03-ba28b8d852d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 16 iterations.\n",
      "[[0.08508988 0.01324971 0.03903036 0.00825696 0.00357584]\n",
      " [0.01324971 0.16104275 0.05365139 0.01140209 0.00491765]\n",
      " [0.03903036 0.05365139 0.0375456  0.00624191 0.00269698]\n",
      " [0.00825696 0.01140209 0.00624191 0.00168939 0.00057151]\n",
      " [0.00357584 0.00491765 0.00269698 0.00057151 0.00031549]]\n",
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.4\n",
    "df = pd.read_csv('test5_3.csv')\n",
    "\n",
    "out = pd.read_csv('testout_5.4.csv')\n",
    "sim = simulateNormal(100000, df, fixMethod=higham_psd)\n",
    "res = np.cov(sim)\n",
    "\n",
    "# atol is the absolute tolerance parameter - set to a lower tolerance since the simulation is random.\n",
    "close_elements = np.isclose(out, res, atol=1e-3)\n",
    "\n",
    "print(res)\n",
    "# Test 5.4 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "68a22198-5109-47ac-b37a-7e26c8be9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multivariate PCA Simulation\n",
    "'''\n",
    "def simulatePCA(N, df, mean=None, seed=1234, pctExp=1):\n",
    "    # Error Checking\n",
    "    m, n = df.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    \n",
    "    # Initialize the output\n",
    "    out = np.zeros((N, n))\n",
    "    \n",
    "    # Set mean\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(df)\n",
    "    \n",
    "    # Get the indices that would sort eigenvalues in descending order\n",
    "    indices = np.argsort(eigenvalues)[::-1]\n",
    "    # Sort eigenvalues\n",
    "    eigenvalues = eigenvalues[indices]\n",
    "    # Sort eigenvectors according to the same order\n",
    "    eigenvectors = eigenvectors[:, indices]\n",
    "    \n",
    "    tv = np.sum(eigenvalues)\n",
    "    posv = np.where(eigenvalues >= 1e-8)[0]\n",
    "    if pctExp <= 1:\n",
    "        nval = 0\n",
    "        pct = 0.0\n",
    "        # How many factors needed\n",
    "        for i in posv:\n",
    "            pct += eigenvalues[i] / tv\n",
    "            nval += 1\n",
    "            if pct >= pctExp:\n",
    "                break\n",
    "    \n",
    "     # If nval is less than the number of positive eigenvalues, truncate posv\n",
    "    if nval < len(posv):\n",
    "        posv = posv[:nval]\n",
    "        \n",
    "    # Filter eigenvalues based on posv\n",
    "    eigenvalues = eigenvalues[posv]\n",
    "    eigenvectors = eigenvectors[:, posv]\n",
    "    \n",
    "    B = eigenvectors @ np.diag(np.sqrt(eigenvalues))\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    rand_normals = np.random.normal(0.0, 1.0, size=(N, len(posv)))\n",
    "    out = np.dot(rand_normals, B.T) + mean\n",
    "    \n",
    "    return out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb5c59f1-26a0-4cd6-bf16-217cb48b8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.5\n",
    "df = pd.read_csv('test5_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout_5.5.csv')\n",
    "sim = simulatePCA(100000, df, pctExp=0.99)\n",
    "res = np.cov(sim)\n",
    "\n",
    "# atol is the absolute tolerance parameter - set to a lower tolerance since the simulation is random.\n",
    "close_elements = np.isclose(out, res, atol=1e-3)\n",
    "\n",
    "\n",
    "# Test 5.5 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ba41caa-ddd0-4eef-8f35-718a854b8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate Return\n",
    "'''\n",
    "\n",
    "# Implement the function to calculate the return\n",
    "def return_calculate(prices, method='ARS', dateColumn='Date'):\n",
    "    # Exclude the date column from the calculations\n",
    "    tickers = [col for col in prices.columns if col != dateColumn]\n",
    "    df = prices[tickers] # The dataframe is now with no date column.\n",
    "    \n",
    "    # Calculate the return using Classical Brownian Motion.\n",
    "    if method == 'CBM':\n",
    "        df = df.diff().dropna()\n",
    "    \n",
    "    # Calculate the return using Arithmetic Return System.\n",
    "    elif method == 'ARS':\n",
    "        df = (df - df.shift(1)) / df.shift(1)\n",
    "        df = df.dropna()\n",
    "        \n",
    "    # Calculate the return using Geometric Brownian Motion.\n",
    "    elif method == 'GBM':\n",
    "        df = np.log(df).diff().dropna()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"method: {method} must be in (\\\"CBM\\\",\\\"ARS\\\",\\\"GBM\\\")\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3c95a1f-da3b-4375-8ab2-2ea4ba56d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# testout_6.1\n",
    "df = pd.read_csv('test6.csv')\n",
    "\n",
    "out = pd.read_csv('test6_1.csv').drop('Date', axis=1)\n",
    "res = return_calculate(df, dateColumn='Date')\n",
    "\n",
    "close_elements = np.allclose(out, res, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 6.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c9a48fff-337a-4036-beb0-afb5aed3b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# testout_6.2\n",
    "df = pd.read_csv('test6.csv')\n",
    "\n",
    "out = pd.read_csv('test6_2.csv').drop('Date', axis=1)\n",
    "res = return_calculate(df, method='GBM', dateColumn='Date')\n",
    "\n",
    "close_elements = np.allclose(out, res, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 6.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a7132c7-bc10-431a-8097-a38e5b92c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "'''\n",
    "Fit the Data with Normal Distribution\n",
    "'''\n",
    "def fit_normal(data):\n",
    "    # Fit the normal distribution to the data\n",
    "    mu, std = norm.fit(data)\n",
    "    return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7890209e-4466-47da-af0e-d5eca2757fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_7.1\n",
    "df = pd.read_csv('test7_1.csv')\n",
    "\n",
    "out = pd.read_csv('testout7_1.csv')\n",
    "res = fit_normal(df)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-3)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 7.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "370d1bcd-59fb-453b-8cbe-fe9165ddc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution\n",
    "'''\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c091f7a0-151f-421f-ba74-248832770570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_7.2\n",
    "df = pd.read_csv('test7_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout7_2.csv')\n",
    "res = fit_general_t(df)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-3)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 7.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aca3d9ca-cd56-4e31-bdc6-d4799807f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution - regression\n",
    "'''\n",
    "def fit_regression_t(df):\n",
    "    Y = df.iloc[:, -1]\n",
    "    X = df.iloc[:, :-1]\n",
    "    betas = MLE_t(X, Y)\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Get the residuals.\n",
    "    e = Y - np.dot(X, betas)\n",
    "\n",
    "    params = t.fit(e)\n",
    "    out = {\"mu\": [params[1]], \n",
    "           \"sigma\": [params[2]], \n",
    "           \"nu\": [params[0]]}\n",
    "    for i in range(len(betas)):\n",
    "        out[\"B\" + str(i)] = betas[i]\n",
    "    out = pd.DataFrame(out)\n",
    "    out.rename(columns={'B0': 'Alpha'}, inplace=True)\n",
    "    return out\n",
    "\n",
    "# The objective negative log-likelihood function (need to be minimized).\n",
    "def MLE_t(X, Y):\n",
    "    X = sm.add_constant(X)\n",
    "    def ll_t(params):\n",
    "        nu, sigma = params[:2]\n",
    "        beta_MLE_t = params[2:]\n",
    "        epsilon = Y - np.dot(X, beta_MLE_t)\n",
    "        # Calculate the log-likelihood\n",
    "        log_likelihood = np.sum(t.logpdf(epsilon, df=nu, loc=mu, scale=sigma))\n",
    "        return -log_likelihood\n",
    "    \n",
    "    beta = np.zeros(X.shape[1])\n",
    "    nu, mu, sigma = 1, 0, np.std(Y - np.dot(X, beta))\n",
    "    params = np.append([nu, sigma], beta)\n",
    "    bnds = ((0, None), (0, None), (None, None), (None, None), (None, None), (None, None))\n",
    "    \n",
    "    # Minimize the log-likelihood to get the beta\n",
    "    res = minimize(ll_t, params, bounds=bnds, options={'disp': True})\n",
    "    beta_MLE = res.x[2:]\n",
    "    return beta_MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8b7bc3e4-753c-4a77-9886-c6232f97b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.21632D+00    |proj g|=  2.91239D+01\n",
      "\n",
      "At iterate    1    f=  2.46768D-01    |proj g|=  3.49674D+01\n",
      "\n",
      "At iterate    2    f= -8.25857D+00    |proj g|=  6.36315D+01\n",
      "\n",
      "At iterate    3    f= -9.00656D+00    |proj g|=  2.01391D+01\n",
      "\n",
      "At iterate    4    f= -9.74769D+00    |proj g|=  1.89823D+01\n",
      "\n",
      "At iterate    5    f= -1.78442D+01    |proj g|=  2.00399D+02\n",
      "\n",
      "At iterate    6    f= -8.53084D+01    |proj g|=  2.05029D+02\n",
      "\n",
      "At iterate    7    f= -1.19412D+02    |proj g|=  1.47445D+02\n",
      "\n",
      "At iterate    8    f= -1.26129D+02    |proj g|=  1.47877D+02\n",
      "\n",
      "At iterate    9    f= -1.33571D+02    |proj g|=  1.20331D+02\n",
      "\n",
      "At iterate   10    f= -1.35629D+02    |proj g|=  1.38188D+02\n",
      "\n",
      "At iterate   11    f= -1.36856D+02    |proj g|=  7.11090D+01\n",
      "\n",
      "At iterate   12    f= -1.36866D+02    |proj g|=  4.68629D+01\n",
      "\n",
      "At iterate   13    f= -1.36932D+02    |proj g|=  3.40481D+00\n",
      "\n",
      "At iterate   14    f= -1.36933D+02    |proj g|=  3.43466D+00\n",
      "\n",
      "At iterate   15    f= -1.36934D+02    |proj g|=  4.03319D+00\n",
      "\n",
      "At iterate   16    f= -1.36940D+02    |proj g|=  1.54564D+01\n",
      "\n",
      "At iterate   17    f= -1.36952D+02    |proj g|=  3.08333D+01\n",
      "\n",
      "At iterate   18    f= -1.36986D+02    |proj g|=  5.66109D+01\n",
      "\n",
      "At iterate   19    f= -1.37066D+02    |proj g|=  9.19996D+01\n",
      "\n",
      "At iterate   20    f= -1.37230D+02    |proj g|=  1.28057D+02\n",
      "\n",
      "At iterate   21    f= -1.37466D+02    |proj g|=  1.26657D+02\n",
      "\n",
      "At iterate   22    f= -1.37515D+02    |proj g|=  1.32287D+02\n",
      "\n",
      "At iterate   23    f= -1.37695D+02    |proj g|=  5.38498D+01\n",
      "\n",
      "At iterate   24    f= -1.37729D+02    |proj g|=  7.10448D+00\n",
      "\n",
      "At iterate   25    f= -1.37732D+02    |proj g|=  6.77345D+00\n",
      "\n",
      "At iterate   26    f= -1.37735D+02    |proj g|=  7.72071D+00\n",
      "\n",
      "At iterate   27    f= -1.37744D+02    |proj g|=  9.33519D+00\n",
      "\n",
      "At iterate   28    f= -1.37760D+02    |proj g|=  9.68815D+00\n",
      "\n",
      "At iterate   29    f= -1.37783D+02    |proj g|=  6.61716D+00\n",
      "\n",
      "At iterate   30    f= -1.37794D+02    |proj g|=  5.54719D+00\n",
      "\n",
      "At iterate   31    f= -1.37805D+02    |proj g|=  4.43079D+00\n",
      "\n",
      "At iterate   32    f= -1.37808D+02    |proj g|=  8.28258D+00\n",
      "\n",
      "At iterate   33    f= -1.37812D+02    |proj g|=  1.36712D+01\n",
      "\n",
      "At iterate   34    f= -1.37815D+02    |proj g|=  1.33533D+01\n",
      "\n",
      "At iterate   35    f= -1.37819D+02    |proj g|=  7.08681D+00\n",
      "\n",
      "At iterate   36    f= -1.37820D+02    |proj g|=  1.57024D-01\n",
      "\n",
      "At iterate   37    f= -1.37820D+02    |proj g|=  9.29617D-02\n",
      "\n",
      "At iterate   38    f= -1.37820D+02    |proj g|=  9.28736D-02\n",
      "\n",
      "At iterate   39    f= -1.37820D+02    |proj g|=  9.25326D-02\n",
      "\n",
      "At iterate   40    f= -1.37820D+02    |proj g|=  9.20807D-02\n",
      "\n",
      "At iterate   41    f= -1.37820D+02    |proj g|=  1.68495D-01\n",
      "\n",
      "At iterate   42    f= -1.37820D+02    |proj g|=  2.81887D-01\n",
      "\n",
      "At iterate   43    f= -1.37820D+02    |proj g|=  4.67244D-01\n",
      "\n",
      "At iterate   44    f= -1.37821D+02    |proj g|=  6.15969D-01\n",
      "\n",
      "At iterate   45    f= -1.37824D+02    |proj g|=  2.47095D+00\n",
      "\n",
      "At iterate   46    f= -1.37829D+02    |proj g|=  1.23375D+00\n",
      "\n",
      "At iterate   47    f= -1.37835D+02    |proj g|=  1.21949D+01\n",
      "\n",
      "At iterate   48    f= -1.37848D+02    |proj g|=  8.07086D+00\n",
      "\n",
      "At iterate   49    f= -1.37870D+02    |proj g|=  2.19979D+00\n",
      "\n",
      "At iterate   50    f= -1.37873D+02    |proj g|=  3.76092D+00\n",
      "\n",
      "At iterate   51    f= -1.37876D+02    |proj g|=  4.78377D-01\n",
      "\n",
      "At iterate   52    f= -1.37878D+02    |proj g|=  3.84509D-01\n",
      "\n",
      "At iterate   53    f= -1.37880D+02    |proj g|=  2.33488D+00\n",
      "\n",
      "At iterate   54    f= -1.37881D+02    |proj g|=  3.45372D-01\n",
      "\n",
      "At iterate   55    f= -1.37881D+02    |proj g|=  2.35949D-01\n",
      "\n",
      "At iterate   56    f= -1.37881D+02    |proj g|=  7.05586D-01\n",
      "\n",
      "At iterate   57    f= -1.37881D+02    |proj g|=  1.37381D+00\n",
      "\n",
      "At iterate   58    f= -1.37881D+02    |proj g|=  2.20492D+00\n",
      "\n",
      "At iterate   59    f= -1.37881D+02    |proj g|=  2.42155D+00\n",
      "\n",
      "At iterate   60    f= -1.37881D+02    |proj g|=  1.32045D+00\n",
      "\n",
      "At iterate   61    f= -1.37881D+02    |proj g|=  5.17201D-01\n",
      "\n",
      "At iterate   62    f= -1.37881D+02    |proj g|=  7.08496D-02\n",
      "\n",
      "At iterate   63    f= -1.37881D+02    |proj g|=  1.19439D-01\n",
      "\n",
      "At iterate   64    f= -1.37881D+02    |proj g|=  4.03872D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     64     77     65     0     0   4.039D-03  -1.379D+02\n",
      "  F =  -137.88135403991987     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "[[ True  True  True  True  True  True  True]]\n",
      "             mu     sigma        nu     Alpha        B1       B2        B3\n",
      "0 -7.971612e-08  0.048548  4.598147  0.042634  0.974815  2.04124  3.154759\n"
     ]
    }
   ],
   "source": [
    "# testout_7.3\n",
    "df = pd.read_csv('test7_3.csv')\n",
    "\n",
    "out = pd.read_csv('testout7_3.csv')\n",
    "res = fit_regression_t(df)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-3)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 7.3 is passed.\n",
    "print(close_elements)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f43ff-fb5d-4e2a-bf00-82f29336c77f",
   "metadata": {},
   "source": [
    "VaR calculation methods (all discussed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "08222fb6-ef1e-4e37-aa56-0fc114a10d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "'''\n",
    "Fit the Data with Normal Distribution\n",
    "'''\n",
    "def fit_normal(data):\n",
    "    # Fit the normal distribution to the data\n",
    "    mu, std = norm.fit(data)\n",
    "    return mu, std\n",
    "\n",
    "\n",
    "'''\n",
    "VaR for Normal Distribution\n",
    "'''\n",
    "\n",
    "def var_normal(data, alpha=0.05):\n",
    "    # Fit the data with normal distribution.\n",
    "    mu, std = fit_normal(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    VaR = -norm.ppf(alpha, mu, std)\n",
    "    \n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    \n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7a783f36-4bb1-47d0-aa2f-93306a7c3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_8.1\n",
    "df = pd.read_csv('test7_1.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_1.csv')\n",
    "\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = var_normal(df, 0.05)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-3)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 8.1 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c7502e4b-6cde-41c3-b884-3f3c731928d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution\n",
    "'''\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "\n",
    "\n",
    "'''\n",
    "VaR for t Distribution\n",
    "''' \n",
    "def var_t(data, alpha=0.05):\n",
    "    # Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    VaR = -t.ppf(alpha, nu, mu, sigma)\n",
    "\n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    \n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eb0bbdfd-5b81-45f2-b239-03244d3956ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_8.2\n",
    "df = pd.read_csv('test7_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_2.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = var_t(df, 0.05)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 8.2 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f0c706b-e071-4b1c-8214-9f438755db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution\n",
    "'''\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "\n",
    "'''\n",
    "VaR for t Distribution simulation\n",
    "''' \n",
    "def var_simulation(data, alpha=0.05, size=10000):\n",
    "    # Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Generate given size random numbers from a t-distribution\n",
    "    random_numbers = t.rvs(df=nu, loc=mu, scale=sigma, size=size)\n",
    "    \n",
    "    return var_t(random_numbers, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8fd875b4-0972-4de2-8379-92767772e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n",
      "   VaR Absolute  VaR Diff from Mean\n",
      "0      0.043077            0.088659\n"
     ]
    }
   ],
   "source": [
    "# testout_8.3 - It depends - 10000 might be not enough\n",
    "df = pd.read_csv('test7_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_3.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = var_simulation(df, 0.05, 10000)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-2)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 8.3 is passed.\n",
    "print(close_elements)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ab74a-a75a-472d-bcea-2f70af6c2ccd",
   "metadata": {},
   "source": [
    "ES calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8a9dcc6f-ad16-4aa9-9f02-6901bb2db104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "\n",
    "'''\n",
    "Fit the Data with Normal Distribution\n",
    "'''\n",
    "def fit_normal(data):\n",
    "    # Fit the normal distribution to the data\n",
    "    mu, std = norm.fit(data)\n",
    "    return mu, std\n",
    "\n",
    "'''\n",
    "VaR for Normal Distribution\n",
    "'''\n",
    "\n",
    "def var_normal(data, alpha=0.05):\n",
    "    # Fit the data with normal distribution.\n",
    "    mu, std = fit_normal(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    VaR = -norm.ppf(alpha, mu, std)\n",
    "    \n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    \n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n",
    "\n",
    "'''\n",
    "ES for Normal Distribution\n",
    "'''\n",
    "\n",
    "def es_normal(data, alpha=0.05):\n",
    "    # Fit the data with normal distribution.\n",
    "    mu, std = fit_normal(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    res = var_normal(data, alpha)\n",
    "    VaR = res.iloc[0, 0]\n",
    "    \n",
    "    # Define the integrand function: x times the PDF of the distribution\n",
    "    def integrand(x, mu, std):\n",
    "        return x * norm.pdf(x, loc=mu, scale=std)\n",
    "    \n",
    "    # Calculate the ES\n",
    "    ES, _ = quad(lambda x: integrand(x, mu, std), -np.inf, -VaR)\n",
    "    ES /= -alpha\n",
    "    \n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    ES_diff = ES + mu\n",
    "    \n",
    "    return pd.DataFrame({\"ES Absolute\": [ES], \n",
    "                         \"ES Diff from Mean\": [ES_diff]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "847145ed-06ac-4207-a84b-df5127a67a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_8.4\n",
    "df = pd.read_csv('test7_1.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_4.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = es_normal(df, 0.05)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-3)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 8.4 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7bdf63b4-2503-4085-a03b-c44648730175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "from scipy.integrate import quad\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution\n",
    "'''\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "\n",
    "'''\n",
    "VaR for t Distribution\n",
    "''' \n",
    "def var_t(data, alpha=0.05):\n",
    "    # Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    VaR = -t.ppf(alpha, nu, mu, sigma)\n",
    "\n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    \n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n",
    "\n",
    "'''\n",
    "ES for t Distribution\n",
    "'''\n",
    "\n",
    "def es_t(data, alpha=0.05):\n",
    "    # Fit the data with normal distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    res = var_t(data, alpha)\n",
    "    VaR = res.iloc[0, 0]\n",
    "    \n",
    "    # Define the integrand function: x times the PDF of the distribution\n",
    "    def integrand(x, mu, sigma, nu):\n",
    "        return x * t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    \n",
    "    # Calculate the ES\n",
    "    ES, _ = quad(lambda x: integrand(x, mu, sigma, nu), -np.inf, -VaR)\n",
    "    ES /= -alpha\n",
    "    \n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    ES_diff = ES + mu\n",
    "    \n",
    "    return pd.DataFrame({\"ES Absolute\": [ES], \n",
    "                         \"ES Diff from Mean\": [ES_diff]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d99598eb-cf00-4eba-b385-25b8aa387773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ES Absolute  ES Diff from Mean\n",
      "0     0.075232           0.121172\n",
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_8.5\n",
    "df = pd.read_csv('test7_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_5.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = es_t(df, 0.05)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-5)  # atol is the absolute tolerance parameter\n",
    "print(res)\n",
    "# Test 8.5 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ead75e58-7475-4a93-91c7-7497e85e5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "'''\n",
    "Fit the Data with t Distribution\n",
    "'''\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "\n",
    "'''\n",
    "VaR for t Distribution\n",
    "''' \n",
    "def var_t(data, alpha=0.05):\n",
    "    # Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Calculate the VaR\n",
    "    VaR = -t.ppf(alpha, nu, mu, sigma)\n",
    "\n",
    "    # Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    \n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n",
    "\n",
    "'''\n",
    "VaR for t Distribution simulation\n",
    "''' \n",
    "\n",
    "def es_simulation(data, alpha=0.05, size=10000):\n",
    "    # Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    # Generate given size random numbers from a t-distribution\n",
    "    random_numbers = t.rvs(df=nu, loc=mu, scale=sigma, size=size)\n",
    "    \n",
    "    return es_t(random_numbers, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c6f2dee9-45d3-4aba-87fb-0c7154caa95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_8.6 - It depends - 10000 might be not enough\n",
    "df = pd.read_csv('test7_2.csv')\n",
    "\n",
    "out = pd.read_csv('testout8_6.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = es_simulation(df, 0.05)\n",
    "\n",
    "close_elements = np.isclose(out, res, atol=1e-2)  # atol is the absolute tolerance parameter\n",
    "\n",
    "# Test 8.6 is passed.\n",
    "print(close_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "88b2b678-31bf-4c71-ae9d-70194e2a8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t\n",
    "\n",
    "'''\n",
    "VaR/ES on 2 levels from simulated values - Copula\n",
    "'''\n",
    "\n",
    "def simulateCopula(portfolio, returns):\n",
    "    portfolio['CurrentValue'] = portfolio['Holding'] * portfolio['Starting Price']\n",
    "    models = {}\n",
    "    uniform = pd.DataFrame()\n",
    "    standard_normal = pd.DataFrame()\n",
    "    \n",
    "    for stock in returns.columns:\n",
    "        # If the distribution for the model is normal, fit the data with normal distribution.\n",
    "        if portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'Normal':\n",
    "            models[stock] = norm.fit(returns[stock])\n",
    "            mu, sigma = norm.fit(returns[stock])\n",
    "            \n",
    "            # Transform the observation vector into a uniform vector using CDF.\n",
    "            uniform[stock] = norm.cdf(returns[stock], loc=mu, scale=sigma)\n",
    "            \n",
    "            # Transform the uniform vector into a Standard Normal vector usig the normal quantile function.\n",
    "            standard_normal[stock] = norm.ppf(uniform[stock])\n",
    "            \n",
    "        # If the distribution for the model is t, fit the data with normal t.\n",
    "        elif portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'T':\n",
    "            models[stock] = t.fit(returns[stock])\n",
    "            nu, mu, sigma = t.fit(returns[stock])\n",
    "            \n",
    "            # Transform the observation vector into a uniform vector using CDF.\n",
    "            uniform[stock] = t.cdf(returns[stock], df=nu, loc=mu, scale=sigma)\n",
    "            \n",
    "            # Transform the uniform vector into a Standard Normal vector usig the normal quantile function.\n",
    "            standard_normal[stock] = norm.ppf(uniform[stock])\n",
    "        \n",
    "    # Calculate Spearman's correlation matrix\n",
    "    spearman_corr_matrix = standard_normal.corr(method='spearman')\n",
    "    \n",
    "    nSim = 10000\n",
    "    \n",
    "    # Use the PCA to simulate the multivariate normal.\n",
    "    simulations = simulatePCA(nSim, spearman_corr_matrix)\n",
    "    simulations = pd.DataFrame(simulations.T, columns=[stock for stock in returns.columns])\n",
    "    \n",
    "    # Transform the simulations into uniform variables using standard normal CDF.\n",
    "    uni = norm.cdf(simulations)\n",
    "    uni = pd.DataFrame(uni, columns=[stock for stock in returns.columns])\n",
    "    \n",
    "    simulatedReturns = pd.DataFrame()\n",
    "    # Transform the uniform variables into the desired data using quantile.\n",
    "    for stock in returns.columns:\n",
    "        # If the distribution for the model is normal, use the quantile of the normal distribution.\n",
    "        if portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'Normal':\n",
    "            mu, sigma = models[stock]\n",
    "            simulatedReturns[stock] = norm.ppf(uni[stock], loc=mu, scale=sigma)\n",
    "            \n",
    "        # If the distribution for the model is t, use the quantile of the t distribution.\n",
    "        elif portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'T':\n",
    "            nu, mu, sigma = models[stock]\n",
    "            simulatedReturns[stock] = t.ppf(uni[stock], df=nu, loc=mu, scale=sigma)\n",
    "    \n",
    "    simulatedValue = pd.DataFrame()\n",
    "    pnl = pd.DataFrame()\n",
    "    # Calculate the daily prices for each stock\n",
    "    for stock in returns.columns:\n",
    "        currentValue = portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        simulatedValue[stock] = currentValue * (1 + simulatedReturns[stock])\n",
    "        pnl[stock] = simulatedValue[stock] - currentValue\n",
    "        \n",
    "    risk = pd.DataFrame(columns = [\"Stock\", \"VaR95\", \"ES95\", \"VaR95_Pct\", \"ES95_Pct\"])\n",
    "    w = pd.DataFrame()\n",
    "\n",
    "    for stock in pnl.columns:\n",
    "        i = risk.shape[0]\n",
    "        risk.loc[i, \"Stock\"] = stock\n",
    "        risk.loc[i, \"VaR95\"] = -np.percentile(pnl[stock], 5)\n",
    "        risk.loc[i, \"VaR95_Pct\"] = risk.loc[i, \"VaR95\"] / portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        risk.loc[i, \"ES95\"] = -pnl[stock][pnl[stock] <= -risk.loc[i, \"VaR95\"]].mean()\n",
    "        risk.loc[i, \"ES95_Pct\"] = risk.loc[i, \"ES95\"] / portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        \n",
    "        # Determine the weights for the two stock\n",
    "        w.at['Weight', stock] = portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0] / portfolio['CurrentValue'].sum()\n",
    "        \n",
    "    # Calculate the total pnl.\n",
    "    pnl['Total'] = 0\n",
    "    for stock in returns.columns:\n",
    "        pnl['Total'] += pnl[stock]\n",
    "    \n",
    "    i = risk.shape[0]\n",
    "    risk.loc[i, \"Stock\"] = 'Total'\n",
    "    risk.loc[i, \"VaR95\"] = -np.percentile(pnl['Total'], 5)\n",
    "    risk.loc[i, \"VaR95_Pct\"] = risk.loc[i, \"VaR95\"] / portfolio['CurrentValue'].sum()\n",
    "    risk.loc[i, \"ES95\"] = -pnl['Total'][pnl['Total'] <= -risk.loc[i, \"VaR95\"]].mean()\n",
    "    risk.loc[i, \"ES95_Pct\"] = risk.loc[i, \"ES95\"] / portfolio['CurrentValue'].sum()\n",
    "\n",
    "\n",
    "        \n",
    "#     w = w.loc['Weight']\n",
    "#     weightedPnl = w * pnl\n",
    "#     weightedPnl['Total'] = 0\n",
    "    \n",
    "#     # Calculate the total PnL for one day.\n",
    "#     for stock in returns.columns:\n",
    "#         weightedPnl['Total'] += weightedPnl[stock]\n",
    "\n",
    "#     # Calculate the portfolio's mean and standard deviation\n",
    "#     portfolioMean = weightedPnl['Total'].mean()\n",
    "#     portfolioStd = weightedPnl['Total'].std()\n",
    "    \n",
    "#     # Determine the risk measure for the whole portfolio\n",
    "#     i = risk.shape[0]\n",
    "#     risk.loc[i, \"Stock\"] = 'Total'\n",
    "#     risk.loc[i, \"VaR95\"] = -norm.ppf(0.05, portfolioMean, portfolioStd)\n",
    "#     risk.loc[i, \"VaR95_Pct\"] = -norm.ppf(0.05, portfolioMean, portfolioStd)risk.loc[i, \"VaR95\"] / portfolio['CurrentValue'].sum()\n",
    "#     risk.loc[i, \"ES95\"] = -weightedPnl['Total'][weightedPnl['Total'] <= -risk.loc[i, \"VaR95\"]].mean()\n",
    "#     risk.loc[i, \"ES95_Pct\"] = risk.loc[i, \"ES95\"] / portfolio['CurrentValue'].sum()\n",
    "        \n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de276b9f-d011-4f42-bc9d-48d3697cd53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stock       VaR95        ES95  VaR95_Pct  ES95_Pct\n",
      "0      A   94.460376  118.289371   0.047230  0.059145\n",
      "1      B  107.880427  151.218174   0.035960  0.050406\n",
      "2  Total  152.565684  199.704532   0.030513  0.039941\n",
      "\n",
      "   Stock       VaR95        ES95 VaR95_Pct  ES95_Pct\n",
      "0      A   93.131154  115.757315  0.046566  0.057879\n",
      "1      B  108.605932  153.976595  0.036202  0.051326\n",
      "2  Total   152.25331  202.826173  0.030451  0.040565\n"
     ]
    }
   ],
   "source": [
    "# testout_9.1\n",
    "df1 = pd.read_csv('test9_1_portfolio.csv')\n",
    "df2 = pd.read_csv('test9_1_returns.csv')\n",
    "\n",
    "out = pd.read_csv('testout9_1.csv')\n",
    "# Calculate the VaR at 5% quantile.\n",
    "res = simulateCopula(df1, df2)\n",
    "\n",
    "# Test 9.1 is passed - within appropriate range.\n",
    "print(f'{out}\\n')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11675d3b",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "432b2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR_normal: -0.09028951366738855\n",
      "ES_normal: 0.11322669274257177\n"
     ]
    }
   ],
   "source": [
    "# Normal distribution with an exponentially weighted variance (lambda=0.97);\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('problem1.csv')\n",
    "# Calculate exponentially weighted moving average (EWMA) of variance\n",
    "lambda_ = 0.97\n",
    "confidence_level = 0.95\n",
    "\n",
    "# \n",
    "m = len(data['x'])\n",
    "weights = np.array([(1 - lambda_) * (lambda_ ** i) for i in range(m)][::-1])\n",
    "ewma_variance = np.sum((data['x'] - data['x'].mean()) ** 2 * weights) / np.sum(weights)\n",
    "ewma_std = np.sqrt(ewma_variance)\n",
    "\n",
    "# VaRES\n",
    "VaR_normal = norm.ppf(1 - confidence_level) * ewma_std\n",
    "ES_normal = (1 / (1 - confidence_level)) * norm.pdf(norm.ppf(1 - confidence_level)) * ewma_std\n",
    "\n",
    "print(f\"VaR_normal: {VaR_normal}\")\n",
    "print(f\"ES_normal: {ES_normal}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "36d04dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR_t:    VaR Absolute  VaR Diff from Mean\n",
      "0      0.076476            0.076382\n",
      "ES_t:    ES Absolute  ES Diff from Mean\n",
      "0     0.113218           0.113124\n"
     ]
    }
   ],
   "source": [
    "# MLE fitted T distribution\n",
    "VaR_T = var_t(data, 0.05)\n",
    "ES_T = es_t(data, 0.05)\n",
    "print(f\"VaR_t: {VaR_T}\")\n",
    "print(f\"ES_t: {ES_T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "86102ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.07598069069686238, -0.11677669788562187)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Historic Simulation\n",
    "# Calculate VaR and ES using historical simulation\n",
    "data_sorted = data['x'].sort_values(ascending=True)\n",
    "VaR_historic = data_sorted.quantile(1 - confidence_level)\n",
    "ES_historic = data_sorted[data_sorted <= VaR_historic].mean()\n",
    "\n",
    "VaR_historic, ES_historic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8b8ca",
   "metadata": {},
   "source": [
    "The VaR and ES under Normal and T distribution is similar (although I assume Var for normal distribution should be smaller than VaR under T distribution). VaR and ES from historical simulation is significantly higher than the other two.\n",
    "\n",
    "The differences in the Value at Risk (VaR) and Expected Shortfall (ES) calculations across the normal distribution, T-distribution, and historical simulation methods stem from the distinct assumptions each method makes about the underlying data distribution and how they handle tail risk. \n",
    "The normal distribution method assumes a symmetric distribution with thin tails, which might not accurately capture the tail risks present in financial data, potentially underestimating the actual market risk. \n",
    "\n",
    "The T-distribution approach, with its degrees of freedom parameter, offers a more flexible model for tail behavior, providing a more accurate risk estimate for financial data with heavy tails. However, the mentioned T-distribution VaR value should reflect maximum expected loss at a given confidence level, indicating sensitivity to tail risk. \n",
    "\n",
    "Historical simulation, not relying on any specific distribution assumption and directly using historical data, reflects observed market conditions and extreme events, offering a straightforward risk measure but limited by the representativeness and range of historical data. The choice between these methods depends on the understanding of data distribution characteristics, the level of concern for extreme events, and risk management preferences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
